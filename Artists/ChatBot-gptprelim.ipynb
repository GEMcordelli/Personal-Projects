{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1f091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan 'Duke  Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'Duke  Duke  Duke of Earl' 'As I walk through this world' 'Nothing can stop the Duke of Earl' 'And-a \n"
     ]
    }
   ],
   "source": [
    "# Make One Big String so the model can learn its cohesive structre for next token, etc.\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Songs/OUTPUTS/AllSongs_Cleaned.csv\", sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Optional: sort if necessary (you want the original order of words)\n",
    "#df = df.sort_values(by=['line_number', 'token_number'])\n",
    "\n",
    "# Concatenate into a single string\n",
    "text = ' '.join(df['chunk_str'].astype(str).tolist())\n",
    "\n",
    "# Let's see what it looks like\n",
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d6bb1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grozz\\miniconda3\\envs\\gemcordelli\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\grozz\\miniconda3\\envs\\gemcordelli\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\grozz\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load pre-trained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Optional: Add special tokens if needed\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have a pad token, just reuse eos\n",
    "\n",
    "# Tokenize your text\n",
    "tokens = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "\n",
    "# Check out the size\n",
    "print(tokens['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a638a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, tokens, block_size=128):\n",
    "        self.input_ids = tokens['input_ids'][0]\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.input_ids[idx:idx + self.block_size]\n",
    "        y = self.input_ids[idx + 1:idx + 1 + self.block_size]\n",
    "        return x, y\n",
    "\n",
    "dataset = LyricsDataset(tokens, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ba7ad27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Step 0 | Loss: 7.1051\n",
      "Epoch 0 | Step 256 | Loss: 5.7210\n",
      "Epoch 0 | Step 512 | Loss: 5.7310\n",
      "Epoch 0 | Step 768 | Loss: 6.0344\n",
      "Epoch 1 | Step 0 | Loss: 3.8072\n",
      "Epoch 1 | Step 256 | Loss: 4.8660\n",
      "Epoch 1 | Step 512 | Loss: 4.8592\n",
      "Epoch 1 | Step 768 | Loss: 5.1227\n",
      "Epoch 2 | Step 0 | Loss: 3.4993\n",
      "Epoch 2 | Step 256 | Loss: 4.0413\n",
      "Epoch 2 | Step 512 | Loss: 3.9977\n",
      "Epoch 2 | Step 768 | Loss: 4.1741\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "from torch.optim import AdamW\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))  # adjust vocab size if you changed it\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop (tiny version)\n",
    "for epoch in range(3):\n",
    "    for i in range(0, len(dataset), 32):  # batch of 32\n",
    "        batch = [dataset[j] for j in range(i, min(i + 32, len(dataset)))]\n",
    "        x_batch = torch.stack([item[0] for item in batch])\n",
    "        y_batch = torch.stack([item[1] for item in batch])\n",
    "\n",
    "        outputs = model(input_ids=x_batch, labels=y_batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 256 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {i} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37c927ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love is  You your  You wanna it' Cause butt  round   round  to waist - round round    To your round' 'm of -ake I it  And't me't  I of\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = \"love is\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "outputs = model.generate(inputs['input_ids'], max_length=50, do_sample=True, top_k=40)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7358523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12647,   705,    35,  ...,   705,  6104,  2330]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fa484d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemcordelli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
